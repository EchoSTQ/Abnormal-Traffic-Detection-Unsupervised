{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CompressionNet:\n",
    "    \"\"\" Compression Network.\n",
    "    This network converts the input data to the representations\n",
    "    suitable for calculation of anormaly scores by \"Estimation Network\".\n",
    "    Outputs of network consist of next 2 components:\n",
    "    1) reduced low-dimensional representations learned by AutoEncoder.\n",
    "    2) the features derived from reconstruction error.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_layer_sizes, activation=tf.nn.tanh):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_layer_sizes : list of int\n",
    "            list of the size of hidden layers.\n",
    "            For example, if the sizes are [n1, n2],\n",
    "            the sizes of created networks are:\n",
    "            input_size -> n1 -> n2 -> n1 -> input_sizes\n",
    "            (network outputs the representation of \"n2\" layer)\n",
    "        activation : function\n",
    "            activation function of hidden layer.\n",
    "            the last layer uses linear function.\n",
    "        \"\"\"\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "\n",
    "    def compress(self, x):\n",
    "        self.input_size = x.shape[1]\n",
    "\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            z = x\n",
    "            n_layer = 0\n",
    "            for size in self.hidden_layer_sizes[:-1]:\n",
    "                n_layer += 1\n",
    "                z = tf.layers.dense(z, size, activation=self.activation,\n",
    "                    name=\"layer_{}\".format(n_layer))\n",
    "\n",
    "            # activation function of last layer is linear\n",
    "            n_layer += 1\n",
    "            z = tf.layers.dense(z, self.hidden_layer_sizes[-1],\n",
    "                name=\"layer_{}\".format(n_layer))\n",
    "\n",
    "        return z\n",
    "\n",
    "    def reverse(self, z):\n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            n_layer = 0\n",
    "            for size in self.hidden_layer_sizes[:-1][::-1]:\n",
    "                n_layer += 1\n",
    "                z = tf.layers.dense(z, size, activation=self.activation,\n",
    "                    name=\"layer_{}\".format(n_layer))\n",
    "\n",
    "            # activation function of last layes is linear\n",
    "            n_layer += 1\n",
    "            x_dash = tf.layers.dense(z, self.input_size,\n",
    "                name=\"layer_{}\".format(n_layer))\n",
    "\n",
    "        return x_dash\n",
    "\n",
    "    def loss(self, x, x_dash):\n",
    "        def euclid_norm(x):\n",
    "            return tf.sqrt(tf.reduce_sum(tf.square(x), axis=1))\n",
    "\n",
    "        # Calculate Euclid norm, distance\n",
    "        norm_x = euclid_norm(x)\n",
    "        norm_x_dash = euclid_norm(x_dash)\n",
    "        dist_x = euclid_norm(x - x_dash)\n",
    "        dot_x = tf.reduce_sum(x * x_dash, axis=1)\n",
    "\n",
    "        # Based on the original paper, features of reconstraction error\n",
    "        # are composed of these loss functions:\n",
    "        #  1. loss_E : relative Euclidean distance\n",
    "        #  2. loss_C : cosine similarity\n",
    "        min_val = 1e-3\n",
    "        loss_E = dist_x  / (norm_x + min_val)\n",
    "        loss_C = 0.5 * (1.0 - dot_x / (norm_x * norm_x_dash + min_val))\n",
    "        return tf.concat([loss_E[:,None], loss_C[:,None]], axis=1)\n",
    "\n",
    "    def extract_feature(self, x, x_dash, z_c):\n",
    "        z_r = self.loss(x, x_dash)\n",
    "        return tf.concat([z_c, z_r], axis=1)\n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\" convert input to output tensor, which is composed of\n",
    "        low-dimensional representation and reconstruction error.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tf.Tensor shape : (n_samples, n_features)\n",
    "            Input data\n",
    "        Results\n",
    "        -------\n",
    "        z : tf.Tensor shape : (n_samples, n2 + 2)\n",
    "            Result data\n",
    "            Second dimension of this data is equal to\n",
    "            sum of compressed representation size and\n",
    "            number of loss function (=2)\n",
    "        x_dash : tf.Tensor shape : (n_samples, n_features)\n",
    "            Reconstructed data for calculation of\n",
    "            reconstruction error.\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"CompNet\"):\n",
    "            # AutoEncoder\n",
    "            z_c = self.compress(x)\n",
    "            x_dash = self.reverse(z_c)\n",
    "\n",
    "            # compose feature vector\n",
    "            z = self.extract_feature(x, x_dash, z_c)\n",
    "\n",
    "        return z, x_dash\n",
    "\n",
    "    def reconstruction_error(self, x, x_dash):\n",
    "        return tf.reduce_mean(tf.reduce_sum(\n",
    "            tf.square(x - x_dash), axis=1), axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
