{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from dagmm.compression_net import CompressionNet\n",
    "from dagmm.estimation_net import EstimationNet\n",
    "from dagmm.gmm import GMM\n",
    "\n",
    "from os import makedirs\n",
    "from os.path import exists, join\n",
    "\n",
    "class DAGMM:\n",
    "    \"\"\" Deep Autoencoding Gaussian Mixture Model.\n",
    "    This implementation is based on the paper:\n",
    "    Bo Zong+ (2018) Deep Autoencoding Gaussian Mixture Model\n",
    "    for Unsupervised Anomaly Detection, ICLR 2018\n",
    "    (this is UNOFFICIAL implementation)\n",
    "    \"\"\"\n",
    "\n",
    "    MODEL_FILENAME = \"DAGMM_model\"\n",
    "    SCALER_FILENAME = \"DAGMM_scaler\"\n",
    "\n",
    "    def __init__(self, comp_hiddens, comp_activation,\n",
    "            est_hiddens, est_activation, est_dropout_ratio=0.5,\n",
    "            minibatch_size=1024, epoch_size=100,\n",
    "            learning_rate=0.0001, lambda1=0.1, lambda2=0.0001,\n",
    "            normalize=True, random_seed=123):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        comp_hiddens : list of int\n",
    "            sizes of hidden layers of compression network\n",
    "            For example, if the sizes are [n1, n2],\n",
    "            structure of compression network is:\n",
    "            input_size -> n1 -> n2 -> n1 -> input_sizes\n",
    "        comp_activation : function\n",
    "            activation function of compression network\n",
    "        est_hiddens : list of int\n",
    "            sizes of hidden layers of estimation network.\n",
    "            The last element of this list is assigned as n_comp.\n",
    "            For example, if the sizes are [n1, n2],\n",
    "            structure of estimation network is:\n",
    "            input_size -> n1 -> n2 (= n_comp)\n",
    "        est_activation : function\n",
    "            activation function of estimation network\n",
    "        est_dropout_ratio : float (optional)\n",
    "            dropout ratio of estimation network applied during training\n",
    "            if 0 or None, dropout is not applied.\n",
    "        minibatch_size: int (optional)\n",
    "            mini batch size during training\n",
    "        epoch_size : int (optional)\n",
    "            epoch size during training\n",
    "        learning_rate : float (optional)\n",
    "            learning rate during training\n",
    "        lambda1 : float (optional)\n",
    "            a parameter of loss function (for energy term)\n",
    "        lambda2 : float (optional)\n",
    "            a parameter of loss function\n",
    "            (for sum of diagonal elements of covariance)\n",
    "        normalize : bool (optional)\n",
    "            specify whether input data need to be normalized.\n",
    "            by default, input data is normalized.\n",
    "        random_seed : int (optional)\n",
    "            random seed used when fit() is called.\n",
    "        \"\"\"\n",
    "        self.comp_net = CompressionNet(comp_hiddens, comp_activation)\n",
    "        self.est_net = EstimationNet(est_hiddens, est_activation)\n",
    "        self.est_dropout_ratio = est_dropout_ratio\n",
    "\n",
    "        n_comp = est_hiddens[-1]\n",
    "        self.gmm = GMM(n_comp)\n",
    "\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epoch_size = epoch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "\n",
    "        self.normalize = normalize\n",
    "        self.scaler = None\n",
    "        self.seed = random_seed\n",
    "\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.sess is not None:\n",
    "            self.sess.close()\n",
    "\n",
    "    def fit(self, x):\n",
    "        \"\"\" Fit the DAGMM model according to the given data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = x.shape\n",
    "\n",
    "        if self.normalize:\n",
    "            self.scaler = scaler = StandardScaler()\n",
    "            x = scaler.fit_transform(x)\n",
    "\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            self.graph = graph\n",
    "            tf.set_random_seed(self.seed)\n",
    "            np.random.seed(seed=self.seed)\n",
    "\n",
    "            # Create Placeholder\n",
    "            self.input = input = tf.placeholder(\n",
    "                dtype=tf.float32, shape=[None, n_features])\n",
    "            self.drop = drop = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "            # Build graph\n",
    "            z, x_dash  = self.comp_net.inference(input)\n",
    "            gamma = self.est_net.inference(z, drop)\n",
    "            self.gmm.fit(z, gamma)\n",
    "            energy = self.gmm.energy(z)\n",
    "\n",
    "            self.x_dash = x_dash\n",
    "\n",
    "            # Loss function\n",
    "            loss = (self.comp_net.reconstruction_error(input, x_dash) +\n",
    "                self.lambda1 * tf.reduce_mean(energy) +\n",
    "                self.lambda2 * self.gmm.cov_diag_loss())\n",
    "\n",
    "            # Minimizer\n",
    "            minimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(loss)\n",
    "\n",
    "            # Number of batch\n",
    "            n_batch = (n_samples - 1) // self.minibatch_size + 1\n",
    "\n",
    "            # Create tensorflow session and initilize\n",
    "            init = tf.global_variables_initializer()\n",
    "\n",
    "            self.sess = tf.Session(graph=graph)\n",
    "            self.sess.run(init)\n",
    "\n",
    "            # Training\n",
    "            idx = np.arange(x.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "\n",
    "            for epoch in range(self.epoch_size):\n",
    "                for batch in range(n_batch):\n",
    "                    i_start = batch * self.minibatch_size\n",
    "                    i_end = (batch + 1) * self.minibatch_size\n",
    "                    x_batch = x[idx[i_start:i_end]]\n",
    "\n",
    "                    self.sess.run(minimizer, feed_dict={\n",
    "                        input:x_batch, drop:self.est_dropout_ratio})\n",
    "\n",
    "                if (epoch + 1) % 100 == 0:\n",
    "                    loss_val = self.sess.run(loss, feed_dict={input:x, drop:0})\n",
    "                    print(\" epoch {}/{} : loss = {:.3f}\".format(epoch + 1, self.epoch_size, loss_val))\n",
    "\n",
    "            # Fix GMM parameter\n",
    "            fix = self.gmm.fix_op()\n",
    "            self.sess.run(fix, feed_dict={input:x, drop:0})\n",
    "            self.energy = self.gmm.energy(z)\n",
    "\n",
    "            tf.add_to_collection(\"save\", self.input)\n",
    "            tf.add_to_collection(\"save\", self.energy)\n",
    "\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Calculate anormaly scores (sample energy) on samples in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array-like, shape (n_samples, n_features)\n",
    "            Data for which anomaly scores are calculated.\n",
    "            n_features must be equal to n_features of the fitted data.\n",
    "        Returns\n",
    "        -------\n",
    "        energies : array-like, shape (n_samples)\n",
    "            Calculated sample energies.\n",
    "        \"\"\"\n",
    "        if self.sess is None:\n",
    "            raise Exception(\"Trained model does not exist.\")\n",
    "\n",
    "        if self.normalize:\n",
    "            x = self.scaler.transform(x)\n",
    "\n",
    "        energies = self.sess.run(self.energy, feed_dict={self.input:x})\n",
    "        return energies\n",
    "\n",
    "    def save(self, fdir):\n",
    "        \"\"\" Save trained model to designated directory.\n",
    "        This method have to be called after training.\n",
    "        (If not, throw an exception)\n",
    "        Parameters\n",
    "        ----------\n",
    "        fdir : str\n",
    "            Path of directory trained model is saved.\n",
    "            If not exists, it is created automatically.\n",
    "        \"\"\"\n",
    "        if self.sess is None:\n",
    "            raise Exception(\"Trained model does not exist.\")\n",
    "\n",
    "        if not exists(fdir):\n",
    "            makedirs(fdir)\n",
    "\n",
    "        model_path = join(fdir, self.MODEL_FILENAME)\n",
    "        self.saver.save(self.sess, model_path)\n",
    "\n",
    "        if self.normalize:\n",
    "            scaler_path = join(fdir, self.SCALER_FILENAME)\n",
    "            joblib.dump(self.scaler, scaler_path)\n",
    "\n",
    "    def restore(self, fdir):\n",
    "        \"\"\" Restore trained model from designated directory.\n",
    "        Parameters\n",
    "        ----------\n",
    "        fdir : str\n",
    "            Path of directory trained model is saved.\n",
    "        \"\"\"\n",
    "        if not exists(fdir):\n",
    "            raise Exception(\"Model directory does not exist.\")\n",
    "\n",
    "        model_path = join(fdir, self.MODEL_FILENAME)\n",
    "        meta_path = model_path + \".meta\"\n",
    "\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            self.graph = graph\n",
    "            self.sess = tf.Session(graph=graph)\n",
    "            self.saver = tf.train.import_meta_graph(meta_path)\n",
    "            self.saver.restore(self.sess, model_path)\n",
    "\n",
    "            self.input, self.energy = tf.get_collection(\"save\")\n",
    "\n",
    "        if self.normalize:\n",
    "            scaler_path = join(fdir, self.SCALER_FILENAME)\n",
    "            self.scaler = joblib.load(scaler_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
